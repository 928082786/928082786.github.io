# Personal Page
<table border="0">
  <tr>
    <td width="60%">
      <h1>Xuan Chen</h1>
      <p><b>Tel: 18142487964</b></p>
      <p><b>Email: chenxuan0839@gmail.com</b></p>
      <p><b>Github:<a href="https://github.com/928082786">
      https://github.com/928082786</a></b> </p>
      <p><b>Address:  Baqiao, Xi'an City, Shaanxi Province</b></p>
      <p><b>Description:  <font face="æ¥·ä½“" size=3pt>I am a graduate student of AFEU. My primary interests are deep learning, adversarial attacks, and backdoor attacks. Besides, the researches of quantum computing and quantum machine learning also attract me much.</font></b></p> 
    </td>
    <td width="40%">
      <img src="me.jpg" width="100%">    
    </td>
  </tr>
</table>



## The recent research

1. Boundary augment: A data augment method to defend poison attack
ðŸ“–[Paper](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12325)

2. Use Procedural Noise to Achieve Backdoor Attack
ðŸ“–[Paper](https://www.researchgate.net/publication/354345187_Use_Procedural_Noise_to_Achieve_Backdoor_Attack)

## Share Pizza
### Adversarial attack

- ðŸš€[<font size=4pt, color=orange>All Adversarial Example Papers</font>](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html)
  > ***This website collects nearly all papers about the adversarial attack. It was built by Nicholas Carlini, a famous researcher in the deep learning security field.***

- ðŸš€[<font size=4pt, color=orange>Trusted-AI/Adversarial Robustness Toolbox </font>](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
  > ***Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security. ART provides tools that enable developers and researchers to defend and evaluate Machine Learning models and applications against the adversarial threats of Evasion, Poisoning, Extraction, and Inference. â€”â€”IBM Trust AI***

- ðŸš€[<font size=4pt, color=orange>Cleverhans</font>](https://github.com/cleverhans-lab/cleverhans)
  > 
